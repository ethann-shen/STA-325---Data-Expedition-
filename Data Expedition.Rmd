---
title: "STA 325 - Data Expedition"
author: "Ethan Shen, Austin Jia, Malavi Ravindran, Steven Herrera"
date: "10/26/2019"
output: rmdformats::readthedown
---

```{r, echo = FALSE, include = FALSE}
# Installing packages
pkgTest <- function(x) {
  if (!require(x,character.only = TRUE)) {
    install.packages(x,dep=TRUE)
  }
}
pkgs <- c("tidyverse", "cowplot", "LaplacesDemon","gamclass", "gam", "splines", "mgcv", "polywog", "DAAG", "gbm", "adabag", "umap")
for (pkg in pkgs) {
  pkgTest(pkg)
}
```

```{r, include = FALSE, echo = FALSE}
library(tidyverse)
library(cowplot)
library(LaplacesDemon)
library(gamclass)
library(gam)
library(splines)
library(mgcv)
library(polywog)
library(DAAG)
library(gbm)
library(adabag)
library(umap)
```

```{r}
train <- read.csv("data-train.csv") 
test <- read.csv("data-test.csv")

train <- train %>%
  mutate(log_first_moment = log(R_moment_1),
         log_second_moment = log(R_moment_2),
         log_third_moment = log(R_moment_3),
         log_fourth_moment = log(R_moment_4))
```

```{r}
set.seed(123)
```

# Exploratory Data Analysis

```{r}
train$Fr <- invlogit(train$Fr)

p1 <- qplot(train$St, bins = 20, 
            main = "Distribution of St", 
            xlab = "St", 
            ylab = "Frequency")
p2 <- qplot(train$Re, bins = 20,
            main = "Distribution of Re", 
            xlab = "Re", 
            ylab = "Frequency")
p3 <- qplot(train$Fr, bins = 20,
            main = "Distribution of Fr", 
            xlab = "Fr", 
            ylab = "Frequency")
cowplot::plot_grid(p1,p2,p3)
```

```{r warning = FALSE, message = FALSE}
m1 <- qplot(R_moment_1, data = train,
            main = "Distribution of Moment 1", 
            xlab = "Moment 1", 
            ylab = "Frequency")

mlog1 <- qplot(log_first_moment, data = train,
               main = "Distribution of log(Moment 1)", 
               xlab = "log(Moment 1)", 
               ylab = "Frequency")
m2 <- qplot(R_moment_2, data = train,
            main = "Distribution of Moment 2", 
            xlab = "Moment 2", 
            ylab = "Frequency")

mlog2 <- qplot(log_second_moment, data = train,
               main = "Distribution of log(Moment 2)", 
               xlab = "log(Moment 2)", 
               ylab = "Frequency")
m3 <- qplot(R_moment_3, data = train,
            main = "Distribution of Moment 3", 
            xlab = "Moment 3", 
            ylab = "Frequency")

mlog3 <- qplot(log_third_moment, data = train,
               main = "Distribution of log(Moment 3)", 
               xlab = "log(Moment 3)", 
               ylab = "Frequency")
m4 <- qplot(R_moment_4, data = train,
            main = "Distribution of Moment 4", 
            xlab = "Moment 4", 
            ylab = "Frequency")

mlog4 <- qplot(log_fourth_moment, data = train,
               main = "Distribution of log(Moment 4)", 
               xlab = "log(Moment 4)", 
               ylab = "Frequency")

cowplot::plot_grid(m1,m2,m3,m4)
cowplot::plot_grid(mlog1, mlog2, mlog3, mlog4)
```


```{r warning = FALSE, message = FALSE}
i1 <- ggplot(data=train, aes(x=Re,y=log_first_moment, color=as.factor(round(Fr, 3)))) + 
  geom_smooth() + 
  labs(y = "log(Moment 1)",
       color = "Fr")
i2 <- ggplot(data=train, aes(x=Re,y=log_second_moment, color=as.factor(round(Fr, 3)))) + 
  geom_smooth() + 
  labs(y = "log(Moment 2)",
       color = "Fr")
i3 <- ggplot(data=train, aes(x=Re,y=log_third_moment, color=as.factor(round(Fr, 3)))) + 
  geom_smooth() + 
  labs(y = "log(Moment 3)",
       color = "Fr")
i4 <- ggplot(data=train, aes(x=Re,y=log_fourth_moment, color=as.factor(round(Fr, 3)))) + 
  geom_smooth() + 
  labs(y = "log(Moment 4)",
       color = "Fr")

title_i <- ggdraw() + 
  draw_label("Interaction between Re and Fr") +
  theme(plot.margin = margin(0, 0, 0, 7))

plot_i <- plot_grid(
  i1 + theme(legend.position = "none"),
  i2 + theme(legend.position = "none"),
  i3 + theme(legend.position = "none"),
  i4 + theme(legend.position = "none"),
  nrow = 2
)

legend <- get_legend(i1)

with_title <- plot_grid(title_i, plot_i, ncol =1, rel_heights = c(0.1,1))

plot_grid(with_title, legend, rel_widths = c(3, 0.4))
```


# Moment 1 - Steven# Moment 1 - Steven

## General Framework

### Linear Model (log transformations)

To begin our analysis, we will conduct simple linear models and see how well it does in predicting our training response variable, which we will update to be **log(the first moment)**. We will also change our variable `Fr` into a **factor** variable.

```{r}
train <- train %>%
  mutate(log_first_moment = log(R_moment_1))

train_with_factor_one <- train %>%
  mutate(Fr = factor(Fr))

just_re <- train_with_factor_one %>%
  mutate(toget = Re,
         type = "Re") %>%
  select(toget, Fr, log_first_moment, type)

just_st <- train_with_factor_one %>%
  mutate(toget = St,
         type = "St") %>%
  select(toget, Fr, log_first_moment, type)

full_plot_data <- rbind(just_re, just_st)
```

We then plot our variables using manipulation techniques.

```{r warning=FALSE, message=FALSE, fig.width=8, fig.height=3}
p1 <- ggplot(data = full_plot_data %>% filter(type == "Re"), mapping = aes(x = toget, y = log_first_moment, color = Fr)) +
  geom_smooth() +
  labs(title = "Re vs. Logged First Moment",
       x = "Re",
       y = "Logged First Moment") +
  theme(plot.title = element_text(hjust = 0.5,size=14))

p2 <- ggplot(data = full_plot_data %>% filter(type == "St"), mapping = aes(x = toget, y = log_first_moment, color = Fr)) +
  geom_smooth() +
  labs(title = "St vs. Logged First Moment",
       x = "St", 
       y = "Logged First Moment") +
  theme(plot.title = element_text(hjust = 0.5,size=14))

plot_grid(p1, p2, ncol = 2)
```

Seeing this visualization, we could justify there being unique interactions among `Fr` + `Re` and `Fr` and `St`. Let's put these variables and their interaction effects in a linear regression model.

```{r}
lm.fit.1 <- lm(log_first_moment ~ St + Re + Fr + Re*Fr + St*Fr, data = train)
lm.fit.1.just.re <- lm(log_first_moment ~ St + Re + Fr + Re*Fr, data = train)
lm.fit.1.no.interactions <- lm(log_first_moment ~ St + Re + Fr, data = train)
anova(lm.fit.1, lm.fit.1.just.re, lm.fit.1.no.interactions)
```

We aren't sure about the effects of the interaction on our models, but we will remove it for now. We will consider using cross validation for further analysis. We will use five-folds since there isn't much training data to work with here.

```{r}
cv.lm(train, form.lm = formula(log_first_moment ~ St + Re + Fr), m = 5, plotit=FALSE, seed = 123)
```

Our average MSE was 0.368, suggesting that the model may seem really good, but with not much understanding of what that looks like in comparison.

### GAMs

We fit a GAM model without interactions below. 

```{r}
gam.fit.1.1 <- gam(log_first_moment ~ s(St) + Re + Fr, data = train)
summary(gam.fit.1.1)
```

We then build one with the interaction effects.

```{r}
gam.fit.1.2 <- gam(log_first_moment ~ s(St) + Re + Fr + Re:Fr, data = train)
summary(gam.fit.1.2)
```

We conduct an ANOVA test below.

```{r}
anova(gam.fit.1.1, gam.fit.1.2, test = "F")
```

We see that the model without the interactions is better, in terms of not having a siginificant Anova F-Test Statistic.

```{r}
library(mgcv)
gamclass::CVgam(log_first_moment ~ s(log(St)) + Re + Fr + Re * Fr, data = train, nfold = 5, method = "GCV.Cp", seed = 123)
```

Our average MSE was 0.397, suggesting that the model may seem really good, but not as good than our linear model.

### Splines


```{r}
spline.1a <- lm(log_first_moment ~ bs(St) + ns(Re) + ns(Fr), data = train)
spline.1b <- lm(log_first_moment ~ bs(St) + ns(Re) + ns(Fr) + ns(Re*Fr), data = train)
```

```{r}
summary(spline.1a)
AIC(spline.1a)
```

```{r}
summary(spline.1b)
AIC(spline.1b)
```

```{r}
anova(spline.1a, spline.1b, test = "F")
```

The spline model with interactions was better. We'll conduct the MSE below.

```{r}
CVgam(log_first_moment ~ bs(St) + ns(Re) + ns(Fr) + ns(Re*Fr), data = train, nfold = 5, method = "GCV.Cp", seed = 123)
```

Our average MSE was 0.407, suggesting that the model may seem really good, but not as good than our linear model.



### MSE Scores LATEX Table

### Flexibility vs. MSE Table

### Final Insights


```{r}
cv.lm(train, form.lm = formula(log_first_moment ~ St + Re + Fr), m = 5, plotit=FALSE, seed = 123)
print("-----------------")
cv.lm(train, form.lm = formula(log_first_moment ~ St + Re + Fr + Re*Fr), m = 5, plotit=FALSE, seed = 123)
print("-----------------")
gamclass::CVgam(log_first_moment ~ s(log(St)) + Re + Fr, data = train, nfold = 5, method = "GCV.Cp", seed = 123)
print("-----------------")
gamclass::CVgam(log_first_moment ~ s(log(St)) + Re + Fr + Re * Fr, data = train, nfold = 5, method = "GCV.Cp", seed = 123)
print("-----------------")
CVgam(log_first_moment ~ bs(St) + ns(Re) + ns(Fr), data = train, nfold = 5, method = "GCV.Cp", seed = 123)
print("-----------------")
CVgam(log_first_moment ~ bs(St) + ns(Re) + ns(Fr) + ns(Re*Fr), data = train, nfold = 5, method = "GCV.Cp", seed = 123)
```






# Moment 2 - Malavi

We will work with the logged value of the second moment as our response variable.




First, we will see if there are important interactions within the data. 


Next we will try fitting a simple linear regression model, taking into account this interaction effect. 

```{r}
lm.fit.2 <- lm(log_second_moment ~ St + Re + Fr + Re*Fr, data=train)
summary(lm.fit.2)
AIC(lm.fit.2)
```

```{r}
cv.lm(train, form.lm = formula(log_second_moment ~ St + Re + Fr + Re*Fr), m = 5)
```

### Polynomial

```{r}
poly.2 <- cv.polywog(log_second_moment ~ St + Re + Fr,
                  data = train,
                  degrees.cv = 1:4,
                  nfolds = 5,
                  thresh = 1e-4)
print(poly.2)

## Extract best model and bootstrap
poly.2 <- poly.2$polywog.fit
poly.2 <- bootPolywog(poly.2, nboot = 5)
summary(poly.2)
```

Next, we will try fititng a couple of different GAM models.

```{r}
gam.fit.2a <- gam(log_second_moment~ s(St) + Re + Fr, data=train)
summary(gam.fit.2a)
CVgam(log_second_moment~ s(St) + Re + Fr, data = train, nfold = 5, method = "GCV.Cp")
AIC(gam.fit.2a)
```


```{r}
gam.fit.2b <- gam(log_second_moment~ s(St) + Re + Fr + Re:Fr, data=train)
summary(gam.fit.2b)
CVgam(log_second_moment~ s(St) + Re + Fr + Re:Fr, data = train, nfold = 5, method = "GCV.Cp")
AIC(gam.fit.2b)
```


```{r}
anova(gam.fit.2a, gam.fit.2b, test = "F")
```

Thus, we will consider a GAM model with interactions. Now we wil test this against a natural spline. 

```{r}
# gam.fit.2c <- gam(log_second_moment~ ns(St) + Re + Fr + Re:Fr, data=train)
# summary(gam.fit.2c)
# CVgam(log_second_moment ~ ns(St) + Re + Fr + Re:Fr, data = train, nfold = 5, method = "GCV.Cp")
```


```{r}
#anova(gam.fit.2c, gam.fit.2b, test = "F")
```

We will work with the regular, rather than natural spline for St. 


We will also try fitting splines.

```{r}
spline.fit.2a <- lm(log_second_moment ~ ns(log(St) + Re) + ns(Fr), data = train) 
CVgam(log_second_moment ~ ns(log(St) + Re) + ns(Fr), data = train, nfold = 5, method = "GCV.Cp")

spline.fit.2b <- lm(log_second_moment ~ ns(St + Re) + bs(Fr), data = train) 
CVgam(log_second_moment ~ ns(St + Re) + bs(Fr), data = train, nfold = 5, method = "GCV.Cp")

```

```{r}
summary(spline.fit.2a)
AIC(spline.fit.2a)
```

```{r}
summary(spline.fit.2b)
AIC(spline.fit.2b)
```

```{r}
anova(spline.fit.2a, spline.fit.2b, test = "F")
```

Looks like the linear model performs slightly better than the GAM. We will use this as our final model to predict the second moment. 

```{r}
lm.fit.2 <- lm(log(R_moment_2) ~ St + Re + Fr + Re*Fr, data=train)

```


```{r}
lm.predictions.2.log <- predict(lm.fit.2, data = test)
```

```{r}
lm.predictions.2 <- exp(lm.predictions.2.log)
lm.predictions.2
```

# Moment 3 - Austin

### Exploratory Data Analysis

### Base Linear Model

We first try a very crude linear model with no transformations/interactions as a baseline. 



Let's first explore a very crude linear model with no transformations.

```{r}
set.seed(123)
lm.fit.3.1 <- lm(R_moment_3 ~ St + Re + Fr, data = train) 
summary(lm.fit.3.1)
AIC(lm.fit.3.1)
cv.lm(train, form.lm = formula(R_moment_3 ~ St + Re + Fr + Re*Fr), m = 5)
```

 
Initial data exploration in the introduction showcased a long right tail, suggesting that a long transformation on the response to be appropriate. This is a transformation that is inherent in the data, and will be carrying through all the models. The above plot which shows co-movement between Re and Fr suggests that an interaction between the two variables could be appropriate. Let's improve our baseline linear model with these enhancements.

```{r}
set.seed(123)
lm.fit.3.2 <- lm(log_third_moment ~ St + Re + Fr + Re*Fr, data=train)
summary(lm.fit.3.2)
AIC(lm.fit.3.2)
cv.lm(train, form.lm = formula(log_third_moment ~ St + Re + Fr + Re*Fr), m = 5)
```

As we can see from the MSE generated from 5-fold cross validaiton, this is a massive improvement. 

### Polynomial 

```{r}
cv3 <- cv.polywog(log_third_moment ~ St + Re + Fr,
                  data = train,
                  degrees.cv = 1:4,
                  nfolds = 5,
                  thresh = 1e-4)
print(cv3)

## Extract best model and bootstrap
fit3 <- cv3$polywog.fit
fit3 <- bootPolywog(fit3, nboot = 5)
summary(fit3)
```

Here, we explore a polynomial fit. Through bootstrapping, the polywog package selected degree 3. This is a further improvement on the log-linear model we fit previously, with CV MSE at 3.87. Next, we explore GAMs. 

### GAMs

Base GAM model 

```{r}
gam.fit.3.1 <- gam(log_third_moment~ s(St) + Re + Fr, data=train)
gamclass::CVgam(log_third_moment ~ s(log(St)) + Re + Fr, data = train, nfold = 5, method = "GCV.Cp")
AIC(gam.fit.3.1)
summary(gam.fit.3.1)
```

GAM model with interactions

```{r}
gam.fit.3.2 <- gam(log_third_moment~ s(St) + Re + Fr + Re:Fr, data=train)
AIC(gam.fit.3.2)
summary(gam.fit.3.2)
gamclass::CVgam(log_third_moment~ s(St) + Re + Fr + Re:Fr, data = train, nfold = 5, method = "GCV.Cp")

gam.fit.3.2 <- gam(log_third_moment~ s(St) + Re + Fr + Re:Fr, data=train)
summary(gam.fit.3.2)
CVgam(log_third_moment~ s(St) + Re + Fr + Re:Fr, data = train, nfold = 5, method = "GCV.Cp")
gamclass::CVgam(log_third_moment ~ s(log(St)) + Re + Fr + Re * Fr, data = train, nfold = 5, method = "GCV.Cp")
```


```{r}
anova(gam.fit.3.1, gam.fit.3.2, test = "F")
```

The anova is significant at an alpha level of 0.001, so we prefer the GAM model with interactions. The CV generated MSE scores also shows that it outforms the simple linear model. It underperforms the polynomial fit by a small margin, which seems to be a price worth paying if our main objective is interpretability. 

### Splines

We will also try fitting splines.

```{r}
spline.fit.3a <- lm(log_third_moment ~ ns(log(St) + Re) + ns(Fr), data = train) 
spline.fit.3b <- lm(log_third_moment ~ ns(St + Re) + bs(Fr), data = train) 
```

```{r}
summary(spline.fit.3a)
AIC(spline.fit.3a)
CVgam(log_third_moment ~ ns(log(St) + Re) + ns(Fr), data = train, nfold = 5, method = "GCV.Cp")
```

```{r}
summary(spline.fit.3b)
AIC(spline.fit.3b)
CVgam(log_third_moment ~ ns(St + Re) + bs(Fr), data = train, nfold = 5, method = "GCV.Cp")
```

```{r}
anova(spline.fit.3a, spline.fit.3b, test = "F")
```

Looks like the second spline is an improvement. From our AIC 

```{r}
lm.fit.3 <- lm(log(R_moment_3) ~ St + Re + Fr + Re*Fr, data=train)
```


```{r}
lm.predictions.3.log <- predict(lm.fit.3, data = test)
```

```{r}
lm.predictions.3 <- exp(lm.predictions.3.log)
lm.predictions.3
```

### MSE Scores LATEX Table

### Flexibility vs. MSE Table

### Final Insights




# Moment 4 - Ethan 


First, we will see if there are important interactions within the data. 


### Linear Model (log transformations)

```{r}
lm.fit.4 <- lm(log_fourth_moment ~ St + Re + Fr + Re*Fr, data = train) 
summary(lm.fit.4)
AIC(lm.fit.4)
cv.lm(train, form.lm = formula(log_fourth_moment ~ St + Re + Fr + Re*Fr), m = 5)
```

### Polynomial 

```{r}
poly.4 <- cv.polywog(log_fourth_moment ~ St + Re + Fr,
                  data = train,
                  degrees.cv = 1:4,
                  nfolds = 5,
                  thresh = 1e-4)
print(poly.4)

## Extract best model and bootstrap
poly.4 <- poly.4$polywog.fit
poly.4 <- bootPolywog(poly.4, nboot = 5)
summary(poly.4)
```


### GAMs

```{r}
gam.4a <- gam(log_fourth_moment ~ ns(St) + Re + Fr, data = train)
summary(gam.4a)
AIC(gam.4a)

CVgam(log_fourth_moment ~ s(St) + Re + Fr, data = train, nfold = 5, method = "GCV.Cp")

```

Adding the interaction helps. 

Testing is natural spline is btter than smoothing spline. 

```{r}
#GOOD MODEL 
gam.4b <- gam(log_fourth_moment ~ ns(St) + Re + Fr + Re:Fr, data = train)
summary(gam.4b)
AIC(gam.4b)

CVgam(log_fourth_moment ~ ns(St) + Re + Fr + Re:Fr, data = train, nfold = 5, method = "GCV.Cp")
```

```{r}
anova(gam.4a, gam.4b, test = "F")

plot(gam.4b, pages=1, residuals=TRUE, all.terms=TRUE, shade=TRUE, shade.col=2)
```
Natural spline is better. 


### Splines

```{r}
spline.4a <- lm(log_fourth_moment ~ bs(St) + ns(Re) + ns(Fr), data = train) #.4697, AIC 4410.8
CVgam(log_fourth_moment ~ bs(St) + ns(Re) + ns(Fr), data = train, nfold = 5, method = "GCV.Cp")
summary(spline.4a)
AIC(spline.4a)

spline.4b <- lm(log_fourth_moment ~ bs(St) + ns(Re) + ns(Fr) + ns(Re*Fr), data = train) #.4607
CVgam(log_fourth_moment ~ bs(St) + ns(Re) + ns(Fr) + ns(Re*Fr), data = train, nfold = 5, method = "GCV.Cp")
summary(spline.4b)
AIC(spline.4b)


anova(spline.4a, spline.4b, test = "F")
```

```{r}
#data.frame(exp(predict(gam.4c, data = test)))
```
