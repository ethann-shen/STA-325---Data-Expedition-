---
title: "STA 325 - Writeup"
author: "Ethan Shen, Austin Jia, Malavi Ravindran, Steven Herrera"
date: "10/26/2019"
output:  rmdformats::readthedown
---

```{r warning = FALSE, message = FALSE, echo = FALSE}
set.seed(123)
# Installing packages
pkgTest <- function(x) {
  if (!require(x,character.only = TRUE)) {
    install.packages(x,dep=TRUE)
  }
}
pkgs <- c("tidyverse", "cowplot", "LaplacesDemon", "gam", "gamclass")
for (pkg in pkgs) {
  pkgTest(pkg)
}

library(tidyverse)
library(cowplot)
library(LaplacesDemon)
library(gam)
library(gamclass)

train <- read.csv("data-train.csv") 
test <- read.csv("data-test.csv")

train <- train %>%
  mutate(log_first_moment = log(R_moment_1),
         log_second_moment = log(R_moment_2),
         log_third_moment = log(R_moment_3),
         log_fourth_moment = log(R_moment_4))
train$Fr <- invlogit(train$Fr)
test$Fr <- invlogit(test$Fr)
```

# Introduction 

Fluid motion is an incredibly complex topic which has significant applications in fields such as engineering, astrophysics, and climatology. Of particular importance is the concept of turbulence which, though easily observable, has been termed the “last great unsolved problem in classical physics.” Metrics such as the Reynolds number (`Re`), which measures the intensity of a turbulent flow, Froud number (`Fr`), which quantifies gravitational acceleration, and Stokes number (`St`), which describes the size density of particles, can be used to bolster understanding of the particles in turbulence.

Our goal throughout this data expedition was two fold. From an inferential perspective, we wanted to understand how each of the measurements mentioned above (`Re`, `Fr`, `St`) affects the probability distribution for particle cluster volumes. From a predictive standpoint, we sought to build a model that would best predict the particle cluster volume distribution from observed values of each of these three parameters. In order to achieve the simultaneous objectives of interpretability and predictive performance, our group was careful in considering highly complex models. When building models for each of the four moments, we tried a handful of modelling methods (scaling in low to high complexity) on our training data, and produced both indirect and direct estimates of testing error from AIC and cross validation for each of these methods.  We then made decisions on which model was most effective by considering both interpretability and our testing error estimates. Our reason for utilizing AIC and cross validation error as a measure of predictive performance was due to our lack of labeled testing data on which we could directly obtain a measure of testing error. 

# Methodology 

## EDA

As mentioned in our introduction, in our attempt to model the probability distribution for particle cluster volumes, we decided on doing so through four separate models, one for each of the first four raw moments. However, prior to separating our models by moment as response variable, we wanted to explore general trends in our data that would inform any necessary transformations and/or interactions between the parameters. First, we knew that we needed to transform the Fr variable, as it contained infinity values. In order to do so, we used the inverse logit function ($\frac{e^x}{1+e^x}$), which transforms any real number $x$ into a value in the interval [0,1]. 

Next, we visualized the distribution of the four moments as histograms to see if any transformations on our response variables were necessary.

```{r warning = FALSE, message = FALSE, echo = FALSE}
m1 <- qplot(R_moment_1, data = train,
            main = "Distribution of Moment 1", 
            xlab = "Moment 1", 
            ylab = "Frequency")

mlog1 <- qplot(log_first_moment, data = train,
               main = "Distribution of log(Moment 1)", 
               xlab = "log(Moment 1)", 
               ylab = "Frequency")
m2 <- qplot(R_moment_2, data = train,
            main = "Distribution of Moment 2", 
            xlab = "Moment 2", 
            ylab = "Frequency")

mlog2 <- qplot(log_second_moment, data = train,
               main = "Distribution of log(Moment 2)", 
               xlab = "log(Moment 2)", 
               ylab = "Frequency")
m3 <- qplot(R_moment_3, data = train,
            main = "Distribution of Moment 3", 
            xlab = "Moment 3", 
            ylab = "Frequency")

mlog3 <- qplot(log_third_moment, data = train,
               main = "Distribution of log(Moment 3)", 
               xlab = "log(Moment 3)", 
               ylab = "Frequency")
m4 <- qplot(R_moment_4, data = train,
            main = "Distribution of Moment 4", 
            xlab = "Moment 4", 
            ylab = "Frequency")

mlog4 <- qplot(log_fourth_moment, data = train,
               main = "Distribution of log(Moment 4)", 
               xlab = "log(Moment 4)", 
               ylab = "Frequency")

cowplot::plot_grid(m1,m2,m3,m4)
```

From this, we noticed a severe right skew in the distributions of all four moments. To combat this, we decided to do a log transformation of each of the moments and use these logged moments as  our response variables in each of the models that we built. We can see that it results in more “normal” looking distributions.

```{r warning = FALSE, message = FALSE, echo = FALSE}
cowplot::plot_grid(mlog1, mlog2, mlog3, mlog4)
```

Next, we wanted to explore potential interactions that would be influential within our models. Below we visualize the interaction between `Re` and `Fr` as it pertains to each moment.

```{r warning = FALSE, message = FALSE, echo = FALSE}
i1 <- ggplot(data=train, aes(x=Re,y=log_first_moment, color=as.factor(round(Fr, 3)))) + 
  geom_smooth() + 
  labs(y = "log(Moment 1)",
       color = "Fr")
i2 <- ggplot(data=train, aes(x=Re,y=log_second_moment, color=as.factor(round(Fr, 3)))) + 
  geom_smooth() + 
  labs(y = "log(Moment 2)",
       color = "Fr")
i3 <- ggplot(data=train, aes(x=Re,y=log_third_moment, color=as.factor(round(Fr, 3)))) + 
  geom_smooth() + 
  labs(y = "log(Moment 3)",
       color = "Fr")
i4 <- ggplot(data=train, aes(x=Re,y=log_fourth_moment, color=as.factor(round(Fr, 3)))) + 
  geom_smooth() + 
  labs(y = "log(Moment 4)",
       color = "Fr")

title_i <- ggdraw() + 
  draw_label("Interaction between Re and Fr") +
  theme(plot.margin = margin(0, 0, 0, 7))

plot_i <- plot_grid(
  i1 + theme(legend.position = "none"),
  i2 + theme(legend.position = "none"),
  i3 + theme(legend.position = "none"),
  i4 + theme(legend.position = "none"),
  nrow = 2
)

legend <- get_legend(i1)

with_title <- plot_grid(title_i, plot_i, ncol =1, rel_heights = c(0.1,1))

plot_grid(with_title, legend, rel_widths = c(3, 0.4))
```

As we can see above, there seems to be a notable interactive effect between `Re` and `Fr` for the second, third, and fourth moments. This does not seem to be the case in the first, where varied values of `Fr` do not yield markedly different slopes on the plot. Thus, in the cases of the second, third, and fourth moments, we will consider this interaction as we build models.

Something that is important to note is that, although our `Re` and `Fr` variables each take only one of three values, we are not treating them as categorical. If one of our goals is to be able to do prediction given any set of parameter values (perhaps `Re` and `Fr` values that are not strictly one of the three we see in the given training data), then it is important to consider these values as continuous. We will now delve in depth into our model selection process.  

## Model Selection

When deciding on a final model for each of the moments, we tested a handful of models with varying complexities and interpretabilities.  Specifically, for each moment, we tested various linear, polynomial, spline, and GAM models. We created models with and without interactions, and then conducted ANOVA tests to determine that models with interactions are generally better (except for the first moment). We avoided testing out more complex models, such as random forest, for the reason that it would provide a very limited scope from an inferential standpoint. 

From here, we made comparisons in both AIC and cross validation error between different classes of models. Our rationale for choosing 5 fold cross validation (as opposed to 10) was the small size of the training dataset, which contained only 89 observations. Below is a summary of the performances of each model we tested, for each of the four moments.

# Results 

## Moment 1

```{r warning = FALSE, message = FALSE, echo = FALSE}
df.m1 <- data.frame(CV.Error.Moment1 = c(0.368, 0.411, 0.397, 0.426, 0.407), AIC.Moment1 = c(167, 171, 169, 170, 168))
row.names(df.m1) <- c("Log Linear", "GAM (without interactions)", "GAM (with interaction)", "Spline 1", "Spline 2")
df.m1
```

```{r warning = FALSE, message = FALSE, echo = FALSE}
lm.fit.1 <- lm(log_first_moment ~ St + Re + Fr, data = train)
summary(lm.fit.1)

fit.1 <- exp(predict(lm.fit.1, newdata = test, type = "response", se.fit = TRUE)$fit)

se.fit.1 <- exp(predict(lm.fit.1, newdata = test, type = "response", se.fit = TRUE)$se.fit)

lower.1 <- fit.1 - (1.96 * se.fit.1)
upper.1 <- fit.1 + (1.96 * se.fit.1)
```

## Moment 2

```{r warning = FALSE, message = FALSE, echo = FALSE}
df.m2 <- data.frame(CV.Error.Moment2 = c(4.71, 5.75, 5.09, 6.00, 4.07), AIC.Moment2 = c(395, 406, 390, 415, 380))
row.names(df.m2) <- c("Log Linear", "GAM (without interactions)", "GAM (with interaction)", "Spline 1", "Spline 2")
df.m2
```

We are using the GAM with interactions as our final model for moment 2. Even though the error is slightly lower for the second spline, a GAM is more interpretable than a spline. 

```{r }
gam.fit.2 <- gam(log_second_moment~ s(St) + Re + Fr + Re:Fr, data=train)
summary(gam.fit.2)

pred.2 <- predict(gam.fit.2, newdata = test, se.fit = TRUE)$fit
fit.2 <- exp(pred.2)

se.2 <- predict(gam.fit.2, newdata = test, se.fit = TRUE)$se.fit
se.fit.2 <- exp(se.2)

lower.2 <- fit.2 - (1.96 * se.fit.2)
upper.2 <- fit.2 + (1.96 * se.fit.2)
```

## Moment 3

```{r warning = FALSE, message = FALSE, echo = FALSE}
df.m3 <- data.frame(CV.Error.Moment3 = c(15.7, 16.3, 13.3, 18.0, 11.0), AIC.Moment3 = c(498, 512, 497, 516, 474))
row.names(df.m3) <- c("Log Linear", "GAM (without interactions)", "GAM (with interaction)", "Spline 1", "Spline 2")
df.m3
```

```{r warning = FALSE, message = FALSE, echo = FALSE}
gam.fit.3 <- gam(log_third_moment~ s(log(St)) + Re + Fr + Re:Fr, data=train)
summary(gam.fit.3)

pred.3 <- predict(gam.fit.3, newdata = test, type = "response", se.fit = TRUE)$fit
fit.3 <- exp(pred.3)

se.3 <- predict(gam.fit.3, newdata = test, type = "response", se.fit = TRUE)$se.fit
se.fit.3 <- exp(se.3)

lower.3 <- fit.3 - (1.96 * se.fit.3)
upper.3 <- fit.3 + (1.96 * se.fit.3)
```


## Moment 4

```{r warning = FALSE, message = FALSE, echo = FALSE}
df.m4 <- data.frame(CV.Error.Moment4 = c(30.6, 38.8, 32.1, 37.8, 32.1), AIC.Moment4 = c(562, 578, 562, 578, 562))
row.names(df.m4) <- c("Log Linear", "GAM (without interactions)", "GAM (with interaction)", "Spline 1", "Spline 2")
df.m4
```

```{r warning = FALSE, message = FALSE, echo = FALSE}
gam.fit.4 <- gam(log_fourth_moment ~ ns(St) + Re + Fr + Re:Fr, data = train)
summary(gam.fit.4)

pred.4 <- predict(gam.fit.4, newdata = test, type = "response", se.fit = TRUE)$fit
fit.4 <- exp(pred.4)

se.4 <- predict(gam.fit.4, newdata = test, type = "response", se.fit = TRUE)$se.fit
se.fit.4 <- exp(se.4)

lower.4 <- fit.4 - (1.96 * se.fit.4)
upper.4 <- fit.4 + (1.96 * se.fit.4)
```

```{r}
pred <- data.frame(fit.1 = fit.1,
                   fit.2 = fit.2, 
                   fit.3 = fit.3, 
                   fit.4 = fit.4,
                   se.fit.1 = se.fit.1, 
                   se.fit.2 = se.fit.2, 
                   se.fit.3 = se.fit.3,
                   se.fit.4 = se.fit.4,
                   lower.1 = lower.1,
                   lower.2 = lower.2,
                   lower.3 = lower.3,
                   lower.4 = lower.4,
                   upper.1 = upper.1,
                   upper.2 = upper.2,
                   upper.3 = upper.3,
                   upper.4 = upper.4)


test <- cbind(test, pred)
write_csv(test, "test.holdout.csv")
```



# Conclusion 